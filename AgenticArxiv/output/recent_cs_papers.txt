================================================================================
ArXiv 计算机科学论文列表
生成时间: 2026-02-21 02:11:39
论文数量: 5
================================================================================

论文 1: Sink-Aware Pruning for Diffusion Language Models
------------------------------------------------------------
ID: 2602.17664v1
作者: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen
发表时间: 2026-02-19 18:59:50
更新时间: 2026-02-19 18:59:50
主要分类: cs.CL
所有分类: cs.CL, cs.AI, cs.LG
PDF链接: https://arxiv.org/pdf/2602.17664v1
备注: Code at: https://github.com/VILA-Lab/Sink-Aware-Pruning
摘要: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typica...
相关链接:
  - https://arxiv.org/abs/2602.17664v1
  - https://arxiv.org/pdf/2602.17664v1

================================================================================

论文 2: CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts
------------------------------------------------------------
ID: 2602.17663v1
作者: Juri Opitz, Corina Raclé, Emanuela Boros, Andrianos Michail, Matteo Romanello, Maud Ehrmann, Simon Clematide
发表时间: 2026-02-19 18:59:44
更新时间: 2026-02-19 18:59:44
主要分类: cs.AI
所有分类: cs.AI, cs.CL, cs.IR
PDF链接: https://arxiv.org/pdf/2602.17663v1
备注: ECIR 2026. CLEF Evaluation Lab. Registration DL: 2026/04/23. Task Homepage at https://hipe-eval.github.io/HIPE-2026/
摘要: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series tow...
相关链接:
  - https://arxiv.org/abs/2602.17663v1
  - https://arxiv.org/pdf/2602.17663v1

================================================================================

论文 3: MARS: Margin-Aware Reward-Modeling with Self-Refinement
------------------------------------------------------------
ID: 2602.17658v1
作者: Payel Bhattacharjee, Osvaldo Simeone, Ravi Tandon
发表时间: 2026-02-19 18:59:03
更新时间: 2026-02-19 18:59:03
主要分类: cs.LG
所有分类: cs.LG, cs.AI, cs.IT
PDF链接: https://arxiv.org/pdf/2602.17658v1
摘要: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models re...
相关链接:
  - https://arxiv.org/abs/2602.17658v1
  - https://arxiv.org/pdf/2602.17658v1

================================================================================

论文 4: Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting
------------------------------------------------------------
ID: 2602.17645v1
作者: Xiaohan Zhao, Zhaoyi Li, Yaxin Luo, Jiacheng Cui, Zhiqiang Shen
发表时间: 2026-02-19 18:54:32
更新时间: 2026-02-19 18:54:32
主要分类: cs.LG
所有分类: cs.LG, cs.AI, cs.CL, cs.CV
PDF链接: https://arxiv.org/pdf/2602.17645v1
备注: Code at: https://github.com/vila-lab/M-Attack-V2
摘要: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches...
相关链接:
  - https://arxiv.org/abs/2602.17645v1
  - https://arxiv.org/pdf/2602.17645v1

================================================================================

论文 5: FAMOSE: A ReAct Approach to Automated Feature Discovery
------------------------------------------------------------
ID: 2602.17641v1
作者: Keith Burghardt, Jienan Liu, Sadman Sakib, Yuning Hao, Bo Li
发表时间: 2026-02-19 18:53:15
更新时间: 2026-02-19 18:53:15
主要分类: cs.LG
所有分类: cs.LG, cs.AI
PDF链接: https://arxiv.org/pdf/2602.17641v1
备注: 23 pages, 6 figures
摘要: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditio...
相关链接:
  - https://arxiv.org/abs/2602.17641v1
  - https://arxiv.org/pdf/2602.17641v1

================================================================================

